
# Document Q&A App – Complete Setup Guide

This guide provides instructions to set up a local AI-powered Question & Answer application using FastAPI (backend), ReactJS (frontend), and Ollama (LLM host). The app allows users to upload PDFs and ask questions based on their contents, with answers generated by a locally hosted language model.

---

## System Requirements

- OS: Windows (WSL2), macOS, or Linux
- RAM: 8–16 GB recommended
- Python: 3.9 or above
- Node.js: v16 or above
- Disk: At least 8GB free (for model download)

---

## 1. Setup Ollama (Local LLM)

Install Ollama from [ollama.com](https://ollama.com/download) and run the following commands:

```bash
ollama serve
ollama pull phi
```

Use `phi` for low-memory systems (8 GB RAM).

If don't want to phi model the remove by 
```bash
ollama rm phi
```

OR 

Use `mistral` for high-performance systems (16 GB RAM).
```bash
ollama pull mistral
```

If don't want to mistral model the remove by 
```bash
ollama rm mistral
```

This will download the model and start the Ollama server on port 11434.
You can check if the server is running by visiting [http://localhost:11434](http://localhost:11434).
You can also check the installed models with:

```bash
ollama list
```
If you encounter issues with port 11434 being in use, you can check which process is using it and terminate it:

```bash
netstat -ano | findstr :11434
taskkill /PID <pid> /F
```
--- 

## 2. Backend Setup (FastAPI + LangChain)

First, ensure you have Python 3.9 or above installed. if not installed then follow below instruction

### Installing Python on D: Drive (Windows)

Follow these steps to install Python on the `D:\python313` drive:

### 1. Download Python Installer
- Visit the official Python website: [https://www.python.org/downloads/](https://www.python.org/downloads/)
- Download the latest Windows installer (e.g., `python-3.13.3-amd64.exe`).

### 2. Run the Installer
- Double-click the downloaded installer.
- On the first screen, **uncheck** the box: `Add Python to PATH`.
- Click **Customize installation**.

### 3. Choose Optional Features
- Leave the default options selected.
- Click **Next**.

### 4. Set Installation Location
- Change the install location to a custom path on your `D:\python313` drive.
- Click **Browse** and select `D:\python313`.
- Click **Install**.
- Wait for the installation to complete.
- Click **Close** when done.

### 5. Add Python to System Environment PATH (if not already added)

If Python is not recognized globally, add it manually:

### Steps:
1. Press `Win + S`, search for **"Environment Variables"**, and open:
   **Edit the system environment variables** → click **Environment Variables**.

2. Under **System variables**, select the `Path` variable and click **Edit**.

3. Click **New** and add the following two paths:
   ```
   D:\Python313\
   D:\Python313\Scripts\
   ```

   ```bash
Navigate to the `D:\python313` folder and follow these steps:
python -m venv venv
./venv/Scripts/activate  # or venv/bin/activate on Linux/Mac
# source venv/bin/activate  # or venv\Scripts\activate on Windows
```
4. Click **OK** to save all changes.

5. Restart any open Git Bash windows.

### 5. Verify Installation
- Open a Git Bash or Command Prompt.
- Type `python --version` to check if Python is installed correctly.
- Type `pip --version` to check if pip is installed correctly.
- Type `python -m pip --version` to check if pip is installed correctly.

Then, clone the repository.
```bash
Navigate to the `backend/` folder and follow these steps:
pip install -r requirements.txt
# Upgrade pip
python -m pip install --upgrade pip
pip install sentence-transformers torch uvicorn fastapi pytest
pip install -U langchain-huggingface
uvicorn main:app --reload
```

This starts the API server at [http://localhost:8000](http://localhost:8000).

---

## 3. Frontend Setup (React)

Navigate to the `frontend/` folder:

```bash
npm install
npm start
```

Your React app will run on [http://localhost:3000](http://localhost:3000).

---

## 4. Features & Workflow

- Upload PDFs to backend (stored locally and parsed into text)
- Text is chunked and embedded using `sentence-transformers`
- Embeddings are stored in FAISS vector database
- When a user asks a question, relevant chunks are retrieved
- Chunks + question are passed to the local LLM via Ollama
- Model generates the answer and returns it to the frontend

---

## 5. API Endpoints

- `POST /upload/` – Upload and index a PDF  
- `POST /ask/` – Ask a question and get an answer

---

## 6. Troubleshooting

Check if Ollama is running:

```bash
curl http://localhost:11434
```

See installed models:

```bash
ollama list
```

If port 11434 is already in use:

```bash
netstat -ano | findstr :11434
taskkill /PID <pid> /F
```

---

## 7. Embed Copilot Studio Chatbot

If you want to embed your Microsoft Copilot Studio chatbot into a webpage:
Go to the Copilot Studio, select your environment, and click on "Embed" to get the HTML code.
You can use the following HTML code to embed the chatbot:

Paste into index.js

```html
<!DOCTYPE html>
<html>
  <body>
    <iframe
      src="https://copilotstudio.microsoft.com/environments/Default-.....?__version__=2"
      frameborder="0"
      style="width: 100%; height: 100%;">
    </iframe>
  </body>
</html>
```

---
Final project Structure:

```bash
document-qa-appln/
├── backend/
│   ├── main.py
│   ├── requirements.txt
│   └── uploaded_docs/
├── frontend/
│   ├── public/
│   ├── src/
│   │   └── App.js
│   │   └── index.js
│   ├── package.json
├── README.md
└── .gitignore
```

© 2025 – Document Q&A App Setup Guide
